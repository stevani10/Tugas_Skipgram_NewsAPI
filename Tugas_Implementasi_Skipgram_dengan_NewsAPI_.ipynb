{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Nama : Stevani Dwi Utomo\n",
        "\n",
        "NIM : 24/546969/PPA/06865"
      ],
      "metadata": {
        "id": "fJtP3O0UXNO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install library newsapi dan NLTK\n",
        "!pip install newsapi-python\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T7wchNrF_Dr",
        "outputId": "3eb1940e-be47-41fa-e33f-92af449da9a4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from newsapi-python) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (2025.1.31)\n",
            "Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: newsapi-python\n",
            "Successfully installed newsapi-python-0.2.7\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dot, Flatten\n",
        "import pandas as pd\n",
        "from newsapi import NewsApiClient\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLzqj5wJGLIP",
        "outputId": "5d8db94d-0e27-4cd3-8828-e93b6565efa4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def news_scrape(YOUR_API_KEY, q, from_param, to, language='en', sort_by='popularity', max_page=1):\n",
        "  # Init NewsAPI\n",
        "  newsapi = NewsApiClient(api_key=YOUR_API_KEY)\n",
        "  df_all = pd.DataFrame()\n",
        "  for i in range(1,max_page+1):\n",
        "    all_articles = newsapi.get_everything(q=q, from_param=from_param, to=to,\n",
        "                                          language=language, sort_by=sort_by, page=i)\n",
        "    df_tmp = pd.json_normalize(all_articles['articles'])\n",
        "    df_all = pd.concat([df_all, df_tmp], axis=0).reset_index(drop=True)\n",
        "\n",
        "  df_all = df_all.drop_duplicates()\n",
        "  return df_all\n",
        "\n",
        "# cleansing text\n",
        "def text_cleansing(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Remove the pattern \"[+digits chars]\"\n",
        "    text = re.sub(r'\\[\\+\\d+\\schars\\]', '', text).strip()\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    # Join back to a string\n",
        "    cleaned_text = ' '.join(lemmas)\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "7h65yOBnGNDT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR_API_KEY = \"cf700d4d4a474353b0cdb01a18e8f70a\"\n",
        "\n",
        "df_all = news_scrape(YOUR_API_KEY, q=\"technology\", from_param=\"2025-02-14\", to=\"2025-03-14\",\n",
        "                     language=\"en\", sort_by=\"popularity\", max_page=5)\n",
        "\n",
        "# Pastikan ada data\n",
        "if df_all.empty:\n",
        "    raise ValueError(\"Tidak ada berita yang diambil dari NewsAPI!\")\n",
        "\n",
        "# Terapkan pembersihan teks\n",
        "df_all_cont = df_all[\"content\"].astype(str).apply(text_cleansing)\n",
        "\n",
        "# Gabungkan semua teks menjadi satu dokumen besar\n",
        "corpus = \" \".join(df_all_cont)\n",
        "\n",
        "# Tokenisasi teks\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([corpus])\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1  # +1 untuk padding\n",
        "\n",
        "# Konversi teks ke dalam format angka\n",
        "sequences = tokenizer.texts_to_sequences([corpus])[0]\n",
        "\n",
        "# Validasi: Pastikan ada cukup kata\n",
        "if len(sequences) < 2:\n",
        "    raise ValueError(\"Data tidak cukup untuk Skipgram. Tambahkan lebih banyak berita!\")\n",
        "\n",
        "# Skipgram pairs\n",
        "pairs, labels = skipgrams(sequences, vocabulary_size=vocab_size, window_size=2)\n",
        "\n",
        "# Periksa apakah pairs tidak kosong sebelum unpacking\n",
        "if len(pairs) == 0:\n",
        "    raise ValueError(\"Skipgram tidak menghasilkan pasangan kata. Periksa preprocessing data!\")\n",
        "\n",
        "target_words, context_words = zip(*pairs)\n",
        "\n",
        "# Bangun model Skipgram dengan dua input\n",
        "def build_model(vocab_size, embedding_dim):\n",
        "    target_input = Input(shape=(1,))\n",
        "    context_input = Input(shape=(1,))\n",
        "\n",
        "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1)\n",
        "\n",
        "    target_embedding = embedding(target_input)\n",
        "    context_embedding = embedding(context_input)\n",
        "\n",
        "    dot_product = Dot(axes=2)([target_embedding, context_embedding])\n",
        "    dot_product = Flatten()(dot_product)\n",
        "\n",
        "    model = Model(inputs=[target_input, context_input], outputs=dot_product)\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
        "    return model\n",
        "\n",
        "# Gunakan variasi dimensi embedding\n",
        "embedding_dims = [50, 100, 200]\n",
        "\n",
        "for embedding_dim in embedding_dims:\n",
        "    print(f\"Training model dengan embedding_dim = {embedding_dim}\")\n",
        "\n",
        "    model = build_model(vocab_size, embedding_dim)\n",
        "\n",
        "    target_words = np.array(target_words)\n",
        "    context_words = np.array(context_words)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    model.fit([target_words, context_words], labels, epochs=5, batch_size=4, verbose=1)\n",
        "\n",
        "    # Ambil embedding untuk analisis\n",
        "    embedding_layer = model.layers[2]  # Layer embedding ada di index ke-2\n",
        "    word_embeddings = embedding_layer.get_weights()[0]\n",
        "\n",
        "    # Tampilkan embedding untuk kata 'technology' jika ada dalam vocab\n",
        "    if \"technology\" in word_index:\n",
        "        print(f\"Embedding untuk 'technology': {word_embeddings[word_index['bitcoin']]}\")\n",
        "    else:nb\n",
        "        print(\"'technology' tidak ditemukan dalam vocab.\")\n",
        "\n",
        "print(\"Training selesai!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7Mm2s_oE-hc",
        "outputId": "bb795915-2750-43a5-9cbf-2881f695f6ad"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model dengan embedding_dim = 50\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m30375/30375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 3ms/step - loss: 3.8821\n",
            "Epoch 2/5\n",
            "\u001b[1m30375/30375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 3ms/step - loss: 1.7944\n",
            "Epoch 3/5\n",
            "\u001b[1m30375/30375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 3ms/step - loss: 1.3861\n",
            "Epoch 4/5\n",
            "\u001b[1m30375/30375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 3ms/step - loss: 1.2197\n",
            "Epoch 5/5\n",
            "\u001b[1m30375/30375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 3ms/step - loss: 1.1566\n",
            "Embedding untuk 'technology': [ 0.26697364 -0.08389466  0.18688537  0.21166794  0.13945119  0.31528673\n",
            "  0.15733288 -0.0796235  -0.2004255   0.2130853   0.00213994 -0.30655682\n",
            "  0.23404074  0.02579229  0.09981859  0.13985477  0.04212297 -0.3528547\n",
            " -0.00685172 -0.28958303  0.00352312  0.08599048  0.0708724  -0.18475132\n",
            "  0.05354453  0.08616711 -0.08317644  0.06745519 -0.3158485   0.40982673\n",
            "  0.07193493 -0.05263796 -0.3242715   0.30542025 -0.30222037 -0.07026853\n",
            "  0.13202691  0.42085418 -0.14766654  0.04025715  0.07398832  0.00381349\n",
            " -0.09755365 -0.01969904  0.1675041  -0.20397899 -0.32177308  0.19935483\n",
            "  0.14475025 -0.25608513]\n",
            "Training model dengan embedding_dim = 100\n",
            "Epoch 1/5\n",
            "\u001b[1m30375/30375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 5ms/step - loss: 3.6555\n",
            "Epoch 2/5\n",
            "\u001b[1m30375/30375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 5ms/step - loss: 1.6483\n",
            "Epoch 3/5\n",
            "\u001b[1m30375/30375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 5ms/step - loss: 1.2595\n",
            "Epoch 4/5\n",
            "\u001b[1m30375/30375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 5ms/step - loss: 1.1718\n",
            "Epoch 5/5\n",
            "\u001b[1m30375/30375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 5ms/step - loss: 1.0930\n",
            "Embedding untuk 'technology': [ 0.11006347  0.17217228  0.11896601 -0.00380935 -0.03163921  0.12520723\n",
            "  0.13357349 -0.04764647  0.03445127  0.03431614 -0.10127402 -0.03337365\n",
            " -0.12955222  0.2469402   0.01537452  0.12641138  0.13197751 -0.10806852\n",
            " -0.09433524 -0.08773134 -0.08242309  0.0601811  -0.07937207  0.14188544\n",
            " -0.20963438 -0.0379037   0.10276166  0.1448793  -0.15185145 -0.02865176\n",
            " -0.14084984 -0.1708502  -0.13891646  0.14483646  0.1338205  -0.19984053\n",
            "  0.05915385 -0.00572147 -0.02233494 -0.052997    0.00731625  0.08647327\n",
            " -0.05084419  0.19628195  0.02154305 -0.1390434   0.25290847 -0.21994731\n",
            " -0.17207314  0.12523676 -0.10730556  0.00100943  0.01635843 -0.01385277\n",
            "  0.18866807  0.01198049  0.08452006  0.0857954  -0.04038836  0.12321525\n",
            "  0.20266935 -0.18546648 -0.12046213  0.20459567  0.19227003 -0.1263346\n",
            "  0.29584253  0.12312549  0.10249916 -0.07172359  0.05544147  0.14470512\n",
            " -0.20316838  0.0973511   0.09442802  0.02796299  0.03500681  0.09916311\n",
            "  0.11668074 -0.14446442  0.11782038 -0.05360526 -0.132577    0.17706603\n",
            " -0.20159331 -0.14416273 -0.18311016 -0.03756475 -0.02168903  0.13010168\n",
            " -0.09416464 -0.1800956   0.23983619 -0.00063395 -0.13945405 -0.06512653\n",
            "  0.05041247  0.11174815 -0.03750067 -0.10786687]\n",
            "Training model dengan embedding_dim = 200\n",
            "Epoch 1/5\n",
            "\u001b[1m30375/30375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 8ms/step - loss: 3.5870\n",
            "Epoch 2/5\n",
            "\u001b[1m30375/30375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 7ms/step - loss: 1.6121\n",
            "Epoch 3/5\n",
            "\u001b[1m30375/30375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 8ms/step - loss: 1.2720\n",
            "Epoch 4/5\n",
            "\u001b[1m30375/30375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 8ms/step - loss: 1.1837\n",
            "Epoch 5/5\n",
            "\u001b[1m30375/30375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 8ms/step - loss: 1.1280\n",
            "Embedding untuk 'technology': [-0.01503637  0.17734753 -0.10991232 -0.24654841  0.0038332   0.10632335\n",
            "  0.06291703 -0.18562506  0.06103814  0.12715003  0.01054912  0.11183919\n",
            " -0.00248901 -0.02453569  0.11424214  0.05469085  0.2492923  -0.15102644\n",
            "  0.17949168  0.13461186  0.01720192 -0.0752298  -0.06983083 -0.05594249\n",
            "  0.15394934  0.02055248 -0.07521541 -0.04208217 -0.28370112  0.02990872\n",
            "  0.08693682 -0.0315895  -0.02488278  0.05931614 -0.08085224  0.04416382\n",
            " -0.01076991 -0.10496857 -0.12470346  0.13471664 -0.06349653 -0.12820385\n",
            "  0.08436721  0.10018674  0.06363005 -0.20183246  0.18396266 -0.01696954\n",
            "  0.03478701  0.05831276  0.13078041  0.20717427  0.10895643  0.02055186\n",
            " -0.02773621 -0.0156963   0.05754768 -0.14585336  0.05704309 -0.10434289\n",
            " -0.00965148 -0.12490186 -0.03595899  0.0427217   0.10568535  0.1545002\n",
            "  0.00829668  0.07062893  0.00328431  0.04968678 -0.13021933  0.12807362\n",
            " -0.13869698  0.0318538  -0.23099571  0.1463677   0.16895463  0.07213192\n",
            " -0.00323594  0.12431093 -0.10927907  0.05307823  0.02963915 -0.06270044\n",
            "  0.00920093 -0.1226505  -0.02560612  0.154027    0.04798526 -0.04197052\n",
            " -0.04457332 -0.16602622 -0.1469302   0.14641978  0.00968705 -0.15353586\n",
            " -0.00497973  0.21212225 -0.0662586  -0.08975697  0.13401318 -0.04030679\n",
            " -0.04720435 -0.07678878 -0.00996034 -0.1566179  -0.09212343 -0.05278839\n",
            " -0.16654804 -0.07562669  0.0234174   0.02517052 -0.11639454 -0.03357222\n",
            "  0.05816719 -0.13809782 -0.06287011 -0.04577804 -0.07918869 -0.25638995\n",
            " -0.17842752  0.3056011   0.08516891  0.01854244 -0.08729194  0.03405844\n",
            "  0.25836048  0.04465708  0.05942103 -0.00963862 -0.03258483  0.1786462\n",
            "  0.03339504  0.0865496  -0.18125406 -0.15081151 -0.19862255 -0.23312481\n",
            " -0.08382455 -0.03348394  0.05192173 -0.07552974 -0.03216099  0.07651334\n",
            "  0.01396688 -0.17147702  0.26662922 -0.07837303 -0.01379133  0.13600072\n",
            " -0.1774179   0.05275649 -0.14261883 -0.0121022  -0.04901218  0.17996047\n",
            " -0.06523757  0.0699207  -0.15441333 -0.17451552 -0.03454192 -0.01729602\n",
            " -0.00102968  0.05188403  0.16277097  0.07838262 -0.12692982  0.19892657\n",
            " -0.2060915   0.13282754  0.04290861  0.05335469  0.11988325 -0.11020946\n",
            " -0.01871059  0.03297816 -0.10827661  0.02397672 -0.13145083  0.17933927\n",
            " -0.06672244  0.22496608 -0.08984625  0.13467684 -0.15522395 -0.15969808\n",
            " -0.02903423  0.09330052  0.0771637  -0.08135965 -0.09981904 -0.01699931\n",
            " -0.047891   -0.16460618 -0.13605049  0.0217745  -0.00822594  0.13355647\n",
            " -0.13837339 -0.08093236]\n",
            "Training selesai!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ambil nama layer embedding secara otomatis\n",
        "embedding_layer_name = [layer.name for layer in model.layers if 'embedding' in layer.name][0]\n",
        "\n",
        "# Ambil bobot embedding dari model\n",
        "embeddings = model.get_layer(embedding_layer_name).get_weights()[0]"
      ],
      "metadata": {
        "id": "Mqp6xt4kEw1_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk mendapatkan kata yang mirip berdasarkan vektor embedding\n",
        "def get_similar_words(target_word, top_n=5):\n",
        "    if target_word not in word2idx:\n",
        "        print(f\"'{target_word}' tidak ditemukan dalam vocabulary.\")\n",
        "        return None\n",
        "\n",
        "    word_idx = word2idx[target_word]\n",
        "\n",
        "    # Ambil nama layer embedding yang sesuai\n",
        "    embedding_layer_name = [layer.name for layer in model.layers if 'embedding' in layer.name][0]\n",
        "\n",
        "    # Ambil bobot embedding dari model\n",
        "    embeddings = model.get_layer(embedding_layer_name).get_weights()[0]\n",
        "\n",
        "    # Pastikan indeks kata valid\n",
        "    if word_idx >= len(embeddings):\n",
        "        print(f\"Indeks '{target_word}' di luar jangkauan embedding.\")\n",
        "        return None\n",
        "\n",
        "    word_vec = embeddings[word_idx]\n",
        "\n",
        "    # Hitung kemiripan kosinus (dot product)\n",
        "    similarities = np.dot(embeddings, word_vec)\n",
        "\n",
        "    # Urutkan indeks berdasarkan kemiripan (dari yang paling mirip)\n",
        "    closest_indices = np.argsort(similarities)[::-1][1:top_n+1]\n",
        "\n",
        "    return [idx2word[idx] for idx in closest_indices if idx in idx2word]\n",
        "\n",
        "# Coba cari kata-kata yang mirip dengan 'technology'\n",
        "print(get_similar_words(\"technology\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-ItwxCaGHEi",
        "outputId": "602e67ba-a379-4d4a-ae50-a5b2b38c0c92"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['by', 'first', 'video', 'president', 'tool']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yODizvphXAwb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}